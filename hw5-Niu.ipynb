{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO-4604/5604 HW5: Semi-Supervised Learning \n",
    "## Deadline: Friday, December 14, 6:00pm MT\n",
    "\n",
    "### Solution by: *Ben Niu* (and list any partners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will implement the self-training algorithm for semi-supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to hand in\n",
    "\n",
    "You will submit the assignment on Canvas. Submit a single Jupyter notebook named `hw5lastname.ipynb`, where lastname is replaced with your last name.\n",
    "\n",
    "If you have any output that is not part of your notebook, you may submit that as a separate document, in a single PDF named `hw5lastname.pdf`. \n",
    "\n",
    "When writing code in this notebook, you are encouraged to create additional cells in whatever way makes the presentation more organized and easy to follow. You are allowed to import additional Python libraries.\n",
    "\n",
    "### Submission policies\n",
    "\n",
    "- **Collaboration:** You are allowed to work with up to 3 people besides yourself. You are still expected to write up your own solution. Each individual must turn in their own submission, and list your collaborators after your name.\n",
    "- **Late submissions:** We allow each student to use up to 5 late days over the semester. You have late days, not late hours. This means that if your submission is late by any amount of time past the deadline, then this will use up a late day. If it is late by any amount beyond 24 hours past the deadline, then this will use a second late, and so on. Once you have used up all late days, late assignments will be given at most 80% credit after one day and 60% credit after two days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "You will use a variant of the heart disease dataset from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/heart+Disease). The data was taken from the [University of Granada's datasets for self-labeled learning](http://sci2s.ugr.es/SelfLabeled).\n",
    "\n",
    "The first 13 columns are features (Age, Sex, ChestPainType, RestBloodPressure, SerumCholestoral, FastingBloodSugar, ResElectrocardiographic, MaxHeartRate, ExerciseInduced, Oldpeak, Slope, MajorVessels, Thal).\n",
    "\n",
    "The last column is the class label, either $1$ or $2$ indicating the presence of heart disease in the patient if the label is known, or \"unlabeled\" otherwise. Only 10% of training instances are labeled in this dataset.\n",
    "\n",
    "Run the code below to load the training data (separated as labeled or unlabeled) and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy \n",
    "\n",
    "df_train = pd.read_csv('http://cmci.colorado.edu/classes/INFO-4604/data/heart_train.csv', header=None)\n",
    "df_test = pd.read_csv('http://cmci.colorado.edu/classes/INFO-4604/data/heart_test.csv', header=None)\n",
    "\n",
    "df_labeled = df_train.loc[df_train[13] != 'unlabeled']\n",
    "df_unlabeled = df_train.loc[df_train[13] == 'unlabeled']\n",
    "\n",
    "Y_train_labeled = df_labeled.iloc[0:, -1].values.astype('int')\n",
    "X_train_labeled = df_labeled.iloc[0:, :-1].values\n",
    "X_train_unlabeled = df_unlabeled.iloc[0:, :-1].values\n",
    "\n",
    "Y_test = df_test.iloc[0:, -1].values.astype('int')\n",
    "X_test = df_test.iloc[0:, :-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Self-Training [16 points]\n",
    "\n",
    "Implement the basic self-training algorithm introduced in lecture. Specifically: \n",
    "\n",
    "1. Train a classifier on the labeled training data only.\n",
    "2. Apply the classifier to the unlabeled training data and treat the classifications as labels.\n",
    "4. Combine the original labeled data with the classifications on the unlabeled dataset to create a new training set.\n",
    "5. Train a second classifier on the new training set.\n",
    "\n",
    "You will experiment with four types of classifiers: [`LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), [`MLPClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html), [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html). The first three you have used in previous assignments. `GaussianNB` is `sklearn`'s implementation of Naive Bayes which uses a Gaussian (normal) distribution to model the probabilities of continuous-valued features.\n",
    "\n",
    "Construct the classifiers using the default parameters. Unlike in the previous two assignments, you won't use cross-validation and you won't perform any hyperparameter tuning. You should just build the self-trained classifier on the training data and then evaluate it on the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliverable 1.1: Train each of the four classifiers on the labeled data only. Calculate the test accuracy for each classifier. These will be your baseline accuracies.\n",
    "\n",
    "[results go here]\n",
    "LogisticRegression: Test accuracy: 0.888889\n",
    "DecisionTreeClassifier:Test accuracy: 0.740741\n",
    "MLPClassifier:Test accuracy: 0.888889\n",
    "GaussianNB:Test accuracy: 0.888889\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Deliverable 1.2: Implement the self-training algorithm as described, and calculate the test accuracies for each of the four classifiers. How do the results compare to the accuracies in 1.1?\n",
    "\n",
    "[results go here]\n",
    "LogisticRegression: Test accuracy: 0.925926\n",
    "DecisionTreeClassifier:Test accuracy: 0.740741\n",
    "MLPClassifier:Test accuracy: 0.888889\n",
    "GaussianNB:Test accuracy: 0.888889\n",
    "\n",
    "\n",
    "#### Deliverable 1.3: You should have found that the multilayer perceptron (MLP) had poor accuracy in 1.1, which drops even more after self-training in 1.2. (a) Why do you think MLP performs poorly on this dataset, when it performed the best in HW3? (b) Why do you think performance dropped after self-training with MLP?\n",
    "\n",
    "[response goes here]\n",
    "(a)Because MLP is the most complicated classifier among those classifiers, and MLP need more data to accurately train the classifier. However, the dataset for this assignment is a small dataset, that maybe the reason why it performs poorly on this dataset. \n",
    "(b)Because the self_training use the dataset from the previous trained dataset, if the first classifier is not accurately trained, the second MLP classifier used inaccurately predicted dataset to train itself, therefore performance dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probelm 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# code for 1.1 here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.000000\n",
      "Test accuracy: 0.888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "classifier.fit(X_train_labeled, Y_train_labeled)\n",
    "print(\"Training accuracy: %0.6f\" % accuracy_score(Y_train_labeled, classifier.predict(X_train_labeled)))\n",
    "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1,\n",
       "       1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2,\n",
       "       1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1,\n",
       "       2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1,\n",
       "       1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2,\n",
       "       1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1,\n",
       "       2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1,\n",
       "       1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2,\n",
       "       1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ylog = classifier.predict(X_train_unlabeled)\n",
    "Ylog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.000000\n",
      "Test accuracy: 0.777778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1,\n",
       "       2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2,\n",
       "       1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1,\n",
       "       2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2,\n",
       "       2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 1, 2,\n",
       "       2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn import preprocessing\n",
    "#le = preprocessing.LabelEncoder()\n",
    "#X_train_labeled1= X_train_labeled.astype(int)\n",
    "#Y_train_labeled1= Y_train_labeled.astype(int)\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_train_labeled, Y_train_labeled)\n",
    "print(\"Training accuracy: %0.6f\" % accuracy_score(Y_train_labeled, classifier.predict(X_train_labeled)))\n",
    "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, classifier.predict(X_test)))\n",
    "Ytree = classifier.predict(X_train_unlabeled)\n",
    "Ytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.000000\n",
      "Test accuracy: 0.888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2,\n",
       "       1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2,\n",
       "       1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1,\n",
       "       2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1,\n",
       "       1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
       "       1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2,\n",
       "       1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1,\n",
       "       2, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2,\n",
       "       1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2,\n",
       "       1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes= (100,) , random_state=None)\n",
    "classifier.fit(X_train_labeled, Y_train_labeled)\n",
    "print(\"Training accuracy: %0.6f\" % accuracy_score(Y_train_labeled, classifier.predict(X_train_labeled)))\n",
    "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, classifier.predict(X_test)))\n",
    "Ymlp = classifier.predict(X_train_unlabeled)\n",
    "Ymlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.875000\n",
      "Test accuracy: 0.888889\n"
     ]
    }
   ],
   "source": [
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train_labeled, Y_train_labeled)\n",
    "print(\"Training accuracy: %0.6f\" % accuracy_score(Y_train_labeled, classifier.predict(X_train_labeled)))\n",
    "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, classifier.predict(X_test)))\n",
    "YGa = classifier.predict(X_train_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.000000\n",
      "Test accuracy: 0.925926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# code for 1.2 here\n",
    "import numpy as np\n",
    "Y_newlog = np.concatenate((Y_train_labeled,Ylog),axis=0)\n",
    "X_new = np.concatenate((X_train_labeled,X_train_unlabeled),axis=0)\n",
    "\n",
    "classifier = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "classifier.fit(X_new,Y_newlog )\n",
    "print(\"Training accuracy: %0.6f\" % accuracy_score(Y_newlog, classifier.predict(X_new)))\n",
    "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1,\n",
       "       1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1,\n",
       "       1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2,\n",
       "       2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1,\n",
       "       2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1,\n",
       "       2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2,\n",
       "       1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1,\n",
       "       1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "       2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_newlog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.000000\n",
      "Test accuracy: 0.777778\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Y_newtree = np.concatenate((Y_train_labeled,Ytree),axis=0)\n",
    "X_new = np.concatenate((X_train_labeled,X_train_unlabeled),axis=0)\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X_new,Y_newtree )\n",
    "print(\"Training accuracy: %0.6f\" % accuracy_score(Y_newtree, classifier.predict(X_new)))\n",
    "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, classifier.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.946502\n",
      "Test accuracy: 0.740741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "Y_newmlp = np.concatenate((Y_train_labeled,Ymlp),axis=0)\n",
    "X_new = np.concatenate((X_train_labeled,X_train_unlabeled),axis=0)\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes= (100,) , random_state=None)\n",
    "classifier.fit(X_new, Y_newmlp)\n",
    "print(\"Training accuracy: %0.6f\" % accuracy_score(Y_newmlp, classifier.predict(X_new)))\n",
    "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, classifier.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.954733\n",
      "Test accuracy: 0.888889\n"
     ]
    }
   ],
   "source": [
    "Y_newGa = np.concatenate((Y_train_labeled,YGa),axis=0)\n",
    "X_new = np.concatenate((X_train_labeled,X_train_unlabeled),axis=0)\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_new, Y_newGa)\n",
    "print(\"Training accuracy: %0.6f\" % accuracy_score(Y_newGa, classifier.predict(X_new)))\n",
    "print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, classifier.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier used for initially labeling the data (steps 1-2 of the algorithm above) does not necessarily need to be the same as the second classifier trained on the labels produced by the first classifier (step 4). For example, when label propagation was introduced in lecture, we learned that sometimes it is used for initially inferring labels, but then an additional classifier is trained on the inferred labels. Even though we are not using label propagation in this assignment, we will use the same idea of training a classifier that is not the same classifier as used to originally label the data.\n",
    "\n",
    "For this next task, you should re-run your self-training algorithm where you try different combinations of classifiers to see how this affects test accuracy.\n",
    "\n",
    "#### Deliverable 1.4: Repeat 1.2 for all $4 \\times 3$ combinations of classifiers, such that the first classifier is different from the second classifier.\n",
    "\n",
    "[results go here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8148148148148148, 0.8518518518518519, 0.9259259259259259]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for 1.4 here\n",
    "X_new = np.concatenate((X_train_labeled,X_train_unlabeled),axis=0)\n",
    "Y = [Ymlp,Ytree,YGa]\n",
    "actest = []\n",
    "for d in Y:\n",
    "    Y_newlog4 = np.concatenate((Y_train_labeled,d),axis=0)\n",
    "\n",
    "\n",
    "    classifier = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "    classifier.fit(X_new,Y_newlog4 )\n",
    "    #print(\"Training accuracy: %0.6f\" % accuracy_score(Y_newlog4, classifier.predict(X_new)))\n",
    "    #print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, classifier.predict(X_test)))\n",
    "    print()\n",
    "    actest.append(accuracy_score(Y_test, classifier.predict(X_test)))\n",
    "   \n",
    "actest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9259259259259259, 0.8888888888888888, 0.9629629629629629]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YMLP = [Ytree,YGa,Ylog]\n",
    "actestMLP = []\n",
    "for i in YMLP:\n",
    "    Y_newmlp4 = np.concatenate((Y_train_labeled,i),axis=0)\n",
    "    X_new = np.concatenate((X_train_labeled,X_train_unlabeled),axis=0)\n",
    "\n",
    "    classifier = MLPClassifier(hidden_layer_sizes= (100,) , random_state=None)\n",
    "    classifier.fit(X_new, Y_newmlp4)\n",
    "    #print(\"Training accuracy: %0.6f\" % accuracy_score(Y_newmlp4, classifier.predict(X_new)))\n",
    "    #print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, classifier.predict(X_test)))\n",
    "    actestMLP.append(accuracy_score(Y_test, classifier.predict(X_test)))\n",
    "actestMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8888888888888888, 0.8888888888888888, 0.8888888888888888]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YGA = [Ymlp,Ytree,Ylog]\n",
    "actestGA = []\n",
    "for g in YGA:\n",
    "\n",
    "    Y_newGa4 = np.concatenate((Y_train_labeled,g),axis=0)\n",
    "    X_new = np.concatenate((X_train_labeled,X_train_unlabeled),axis=0)\n",
    "\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(X_new, Y_newGa)\n",
    "    #print(\"Training accuracy: %0.6f\" % accuracy_score(Y_newGa4, classifier.predict(X_new)))\n",
    "    #print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, classifier.predict(X_test)))\n",
    "    actestGA.append(accuracy_score(Y_test, classifier.predict(X_test)))\n",
    "actestGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7037037037037037, 0.7777777777777778, 0.8518518518518519]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YTree = [Ymlp,Ytree,Ylog]\n",
    "actestTree = []\n",
    "for i in YTree:\n",
    "    Y_newtree4 = np.concatenate((Y_train_labeled,i),axis=0)\n",
    "    X_new = np.concatenate((X_train_labeled,X_train_unlabeled),axis=0)\n",
    "    classifier = DecisionTreeClassifier()\n",
    "    classifier.fit(X_new,Y_newtree4 )\n",
    "    #print(\"Training accuracy: %0.6f\" % accuracy_score(Y_newtree, classifier.predict(X_new)))\n",
    "    #print(\"Test accuracy: %0.6f\" % accuracy_score(Y_test, classifier.predict(X_test)))\n",
    "    actestTree.append(accuracy_score(Y_test, classifier.predict(X_test)))\n",
    "actestTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Problem 2: Thresholding [+6 EC points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify your self-training algorithm using the high-confidence variant described in lecture. Specifically, when you add classifications to the training set, only include instances where the classifier had a probability above some threshold. Recall from HW2 that you can obtain classifier probabilities using the `predict_proba` function. This algorithm can be repeated for multiple iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deliverable 2.1: Implement this version of self-training using a threshold probability that can be specified. Repeat problem 1.2 using this new version on each of the four classifiers. Calculate the test accuracy after running this algorithm for 1, 2, and 3 iterations. Experiment with the following probability thresholds: $0.99, 0.9, 0.8, 0.6$.\n",
    "\n",
    "[results go here]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6,\n",
       " 'Test accuracy for LogisticRegression Model: 0.925926',\n",
       " 0.8,\n",
       " 'Test accuracy for LogisticRegression Model: 0.851852',\n",
       " 0.9,\n",
       " 'Test accuracy for LogisticRegression Model: 0.851852',\n",
       " 0.99,\n",
       " 'Test accuracy for LogisticRegression Model: 0.740741']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for 2.1 here\n",
    "def threshold(probs, tau):\n",
    "    return np.where(probs[:,0] > tau,1,2)\n",
    "\n",
    "tau= [0.6,0.8,0.9,0.99]\n",
    "actest = []\n",
    "for t in tau:\n",
    "    \n",
    "#Y_newlog = np.concatenate((Y_train_labeled,Ylog),axis=0)\n",
    "#X_new = np.concatenate((X_train_labeled,X_train_unlabeled),axis=0)\n",
    "    classifier = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "    classifier.fit(X_new,Y_newlog)\n",
    "    actest.append(t)\n",
    "    actest.append(\"Test accuracy for LogisticRegression Model: %0.6f\" %accuracy_score(Y_test, threshold(classifier.predict_proba(X_test),t)))\n",
    "    Y_newlog = threshold(classifier.predict_proba(X_new),t) \n",
    "actest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1,\n",
       "       1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1,\n",
       "       2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1,\n",
       "       1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1,\n",
       "       1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,\n",
       "       2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1,\n",
       "       1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1,\n",
       "       2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1,\n",
       "       2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2,\n",
       "       1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1,\n",
       "       1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 1,\n",
       "       2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_newlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6,\n",
       " 'Test accuracy for DecisionTreeClassifier: 0.777778',\n",
       " 0.8,\n",
       " 'Test accuracy for DecisionTreeClassifier: 0.777778',\n",
       " 0.9,\n",
       " 'Test accuracy for DecisionTreeClassifier: 0.777778',\n",
       " 0.99,\n",
       " 'Test accuracy for DecisionTreeClassifier: 0.777778']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau= [0.6,0.8,0.9,0.99]\n",
    "actest = []\n",
    "for t in tau:\n",
    "#Y_newlog = np.concatenate((Y_train_labeled,Ylog),axis=0)\n",
    "#X_new = np.concatenate((X_train_labeled,X_train_unlabeled),axis=0)\n",
    "    classifier = DecisionTreeClassifier()\n",
    "    classifier.fit(X_new,Y_newtree)\n",
    "    actest.append(t)\n",
    "    actest.append(\"Test accuracy for DecisionTreeClassifier: %0.6f\" %accuracy_score(Y_test, threshold(classifier.predict_proba(X_test),t)))\n",
    "    Y_newtree = threshold(classifier.predict_proba(X_new),t)\n",
    "actest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6,\n",
       " 'Test accuracy for MLPClassifier: 0.777778',\n",
       " 0.8,\n",
       " 'Test accuracy for MLPClassifier: 0.777778',\n",
       " 0.9,\n",
       " 'Test accuracy for MLPClassifier: 0.703704',\n",
       " 0.99,\n",
       " 'Test accuracy for MLPClassifier: 0.629630']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau= [0.6,0.8,0.9,0.99]\n",
    "actest = []\n",
    "for t in tau:\n",
    "#Y_newlog = np.concatenate((Y_train_labeled,Ylog),axis=0)\n",
    "#X_new = np.concatenate((X_train_labeled,X_train_unlabeled),axis=0)\n",
    "    classifier = MLPClassifier()\n",
    "    classifier.fit(X_new,Y_newmlp)\n",
    "    actest.append(t)\n",
    "    actest.append(\"Test accuracy for MLPClassifier: %0.6f\" %accuracy_score(Y_test, threshold(classifier.predict_proba(X_test),t)))\n",
    "    Y_newmlp = threshold(classifier.predict_proba(X_new),t)\n",
    "actest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6,\n",
       " 'Test accuracy for GaussianNB: 0.888889',\n",
       " 0.8,\n",
       " 'Test accuracy for GaussianNB: 0.888889',\n",
       " 0.9,\n",
       " 'Test accuracy for GaussianNB: 0.925926',\n",
       " 0.99,\n",
       " 'Test accuracy for GaussianNB: 0.740741']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau= [0.6,0.8,0.9,0.99]\n",
    "actest = []\n",
    "for t in tau:\n",
    "#Y_newlog = np.concatenate((Y_train_labeled,Ylog),axis=0)\n",
    "#X_new = np.concatenate((X_train_labeled,X_train_unlabeled),axis=0)\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(X_new,Y_newGa)\n",
    "    actest.append(t)\n",
    "    actest.append(\"Test accuracy for GaussianNB: %0.6f\" %accuracy_score(Y_test, threshold(classifier.predict_proba(X_test),t)))\n",
    "    Y_newGa = threshold(classifier.predict_proba(X_new),t)\n",
    "actest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
